# 业务逻辑细节写法说明

_HandleRegister(),_HandleLogIn()里边到底执行什么，是属于业务逻辑代码；写法，大家自己决定
发送数据代码下节实现

```c++
//----------------------------------------------------------------------------------------------------------
//处理各种业务逻辑
bool CLogicSocket::_HandleRegister(lpngx_connection_t pConn,LPSTRUC_MSG_HEADER pMsgHeader,char *pPkgBody,unsigned short iBodyLength)
{
    // ngx_log_stderr(0,"执行了CLogicSocket::_HandleRegister()!");

    // （1）首先判断包体的合法性
    if (pPkgBody == NULL)       // 具体看客户端服务器约定，如果约定这个命令【msgCode】必须带包体，那么如果不带包体，就认为是恶意包，直接不处理
    {
        return false;
    }

    int iRecvLen = sizeof(STRUCT_REGISTER);
    if (iRecvLen != iBodyLength)    // 发送过来的数据结构大小不对，认为是恶意包，直接不处理
    {
        return false;
    }

    // （2）对于同一个用户，可能同时发送过来多个请求，造成多个线程同时为该用户服务，比如以网游为例，用户要在商店中买A物品，又买B物品，而用户的钱 只够买A或者B，不够同时买A和B呢？
    // 那如果用户发送购买命令过来买了一次A，又买了一次B，如果是两个线程来执行同一个用户的这两次不同的购买命令，很可能造成这个用户购买成功了 A，又购买成功了 B
    // 所以为了稳妥起见，针对某个用户的命令，我们一般都要进行互斥，需要增加临界的变量在ngx_connection_s结构中
    Clock lock(&pConn->logicPorcMutex);         // 凡是和本用户有关的访问都要互斥

    // （3）取得了整个发送过来的数据
    LPSTRUCT_REGISTER p_RecvInfo = (LPSTRUCT_REGISTER)pPkgBody;

    // （4）这里可能要考虑根据业务逻辑，进一步判断收到数据的合法性
    // 当前该用户的状态是否适合收到这个数据等等【其实就是登陆验证】
    // 。。。。。。。。

    // （5）给客户端返回数据时，一般也是返回一个结构，这个结构内容具体由客户端/服务器协商，这里我们就可以给客户端也返回同样的 STRUCT_REGISTER 结构来举例
    // LPSTRUCT_REGISTER pFromPkgHeader = (LPSTRUCT_REGISTER)(((char*)pMsgHeader) + m_iLenMsgHeader);      // 指向收到的包的包头，其中数据后续可能要用到
    LPCOMM_PKG_HEADER pPkgHeader;
    CMemory *p_memory = CMemory::GetInstance();
    CCRC32  *p_crc32  = CCRC32::GetInstance();
    int     iSendLen = sizeof(STRUCT_REGISTER);

    // a) 分配要发送出去的包的内存
    char *p_sendbuf = (char *)p_memory->AllocMemory(m_iLenMsgHeader + m_iLenPkgHeader + iSendLen, false);   // 准备发送的格式，这里是消息头 + 包头 + 包体
    // b) 填充消息头
    memcpy(p_sendbuf, pMsgHeader, m_iLenMsgHeader);                     // 消息头直接拷贝到这里
    // c）填充包头
    pPkgHeader = (LPCOMM_PKG_HEADER)(p_sendbuf + m_iLenMsgHeader);      // 指向包头
    pPkgHeader->msgCode = _CMD_REGISTER;                                // 消息代码，可以统一在ngx_logiccomm.h中定义
    pPkgHeader->msgCode = htons(pPkgHeader->msgCode);                   // 主机序转网络序
    pPkgHeader->pkgLen  = htons(m_iLenPkgHeader + iSendLen);            // 整个包的尺寸【包头+包体】

    // d) 填充包体
    LPSTRUCT_REGISTER p_sendInfo = (LPSTRUCT_REGISTER)(p_sendbuf + m_iLenMsgHeader + m_iLenPkgHeader);  // 跳过消息头，跳过包头，就是包体
    // 。。。。。这里可以根据需要，填充要发回给客户端的内容，int类型要使用htonl()转，short类型要使用htons转

    // e) 包体内容全部确定好之后，计算包体的crc32值
    pPkgHeader->crc32   = p_crc32->Get_CRC((unsigned char *)p_sendInfo, iSendLen);
    pPkgHeader->crc32   = htonl(pPkgHeader->crc32);

    //f)发送数据包
    /*msgSend(p_sendbuf);
    //如果时机OK才add_event
    if(ngx_epoll_add_event(pConn->fd,                 //socket句柄
                                0,1,              //读，写 ,这里读为1，表示客户端应该主动给我服务器发送消息，我服务器需要首先收到客户端的消息；
                                0,//EPOLLET,      //其他补充标记【EPOLLET(高速模式，边缘触发ET)】
                                                      //后续因为实际项目需要，我们采用LT模式【水平触发模式/低速模式】
                                EPOLL_CTL_MOD,    //事件类型【增加，还有删除/修改】                                    
                                pConn              //连接池中的连接
                                ) == -1)
                                {
                                    //ngx_log_stderr(0,"111111111111!");
                                }
    */

   /*
    sleep(100);  //休息这么长时间
    //如果连接回收了，则肯定是iCurrsequence不等了
    if(pMsgHeader->iCurrsequence != pConn->iCurrsequence)
    {
        //应该是不等，因为这个插座已经被回收了
        ngx_log_stderr(0,"插座不等,%L--%L",pMsgHeader->iCurrsequence,pConn->iCurrsequence);
    }
    else
    {
        ngx_log_stderr(0,"插座相等哦,%L--%L",pMsgHeader->iCurrsequence,pConn->iCurrsequence);
    }*/
    
    

    return true;
}
bool CLogicSocket::_HandleLogIn(lpngx_connection_t pConn,LPSTRUC_MSG_HEADER pMsgHeader,char *pPkgBody,unsigned short iBodyLength)
{
    ngx_log_stderr(0,"执行了CLogicSocket::_HandleLogIn()!");
    return true;
}

```



# 连接池中连接回收的深度思考

服务器 7*24不间断，服务器稳定性是第一位的；一个服务器，如果不稳定，那么谈别的都是虚的；
稳定性：连接池连接回收问题；
如果客户端【张三】断线，服务器端立即回收连接，这个连接很可能被紧随其后连入的新客户端【李四】所使用，那么这里就很可能产生麻烦；
a)张三 funca(); —执行10秒，服务器从线程池中找一个线程来执行张三的任务；
b)执行到第5秒的时候，张三断线，但是离线这个事，我上面funca函数是感知不到的，因为现在是分配了一个线程去执行该函数
c)张三断线这个事情会被服务器立即感知到，服务器随后调用ngx_close_connection把原来属于张三这个连接池中的连接给回收了；
d)第7秒的时候，李四连上来了，系统会把刚才张三用过的连接池中的连接分配给李四【现在这个连接归李四使用】；
e)10秒钟到了，这个线程很可能会继续操纵 连接【修改读数据】；很可能导致服务器整个崩溃；这种可能性是非常有的；

一个连接，如果我们程序判断这个连接不用了；那么 不 应该把这个连接立即放到空闲队列里，而是 应该放到一个地方；
等待一段时间【60】，60秒之后，我再真正的回收这个连接到 连接池/空闲队列 中去，这种连接才可以真正的分配给其他用户使用；
为什么要等待60秒，就是需要确保即便用户张三真断线了，那么我执行的该用户的业务逻辑也一定能在这个等待时间内全部完成；
这个连接不立即回收是非常重要的，有个时间缓冲非常重要；这个可以在极大程度上确保服务器的稳定；
服务器程序要常年累月的优化；

## 灵活创建连接池

```c++
// 1)epoll功能初始化，子进程中进行，本函数被ngx_worker_process_init()调用
int CSocket::ngx_epoll_init()
{
    // 1）很多内核版本不处理epoll——create参数，知道该参数 > 0 即可
    // 创建一个epoll对象，创建了一个红黑树，还创建了一个双向链表
    m_epollhandle = epoll_create(m_worker_connections);     // 直接以epoll连接的最大项数为参数，肯定满足大于0
    if(m_epollhandle == -1)
    {
        ngx_log_stderr(errno, "CSocket::ngx_epoll_init()中epoll_create()失败。");
        exit(2);    // 这个是致命问题。直接退出系统。资源交由系统进行释放
    }

    // 2）创建连接池【数组】、创建出来，这个东西用于后续处理所有客户端的连接
    initconnection();

    // m_connection_n = m_worker_connections;      // 记录当前连接池中的连接总数
    // // 连接池【数组，每个元素时一个对象】
    // m_pconnections = new ngx_connection_t[m_connection_n];      // new是不可能失败，这里不用判断，如果失败，直接报异常会更好
    // // m_pread_events = new ngx_event_t[m_connection_n];
    // // m_pwrite_events = new mgx_event_t[m_connection_n];
    // // for(int i = 0; i < m_connection_n; i++)
    // // {
    // //     m_pconnections[i].instance = 1; // 失效标志位设置为1,
    // // }

    // int i = m_connection_n;             // 连接池中的连接数
    // lpngx_connection_t next = NULL;
    // lpngx_connection_t c = m_pconnections;  // 连接池数组的首地址

    // // 这个do while循环就是在初始化连接池，并将连接池数组元素通过next指针绑定到一起，形成链表
    // do
    // {
    //     i--;                    // 注意i是数字末尾，从最后遍历，i递减至数组的首个元素

    //     // 从尾部往头部走----------------------
    //     c[i].data = next;       // 设置连接对象的next指针，注意第一次循环时next = NULL
    //     c[i].fd = -1;           // 初始化连接，无socket和该连接池中的连接【对象】绑定
    //     c[i].instance = 1;      // 失效标志位设置为1,【失效】
    //     c[i].iCurrsequence = 0; // 当前序号统一从 0 开始

    //     // -----------------------------------

    //     next = &c[i];           // next指针向前移

    // } while (i);    // 循环直到 i 为 0 .即循环到数组首地址
    // // 注意这里：当这个循环执行完毕后，next的指向现在是指向这个链表的表头

    // m_pfree_connections = next;         // 设置空闲连接链表头指针，因为现在next指向c[0]，注意现在整个链表都是空的
    // m_free_connection_n = m_connection_n;   // 空闲连接链表的长度，因为现在整个链表都是空的，所以这两个参数相等

    // 3）遍历所有监听socket【监听端口】，我们为每个监听socket增加一个 连接池 中的连接。说白了，就是让一个socket和一个内存绑定，以方便记录该socket相关的数据，状态等
    std::vector<lpngx_listening_t>::iterator pos;
    for(pos = m_ListenSocketList.begin(); pos != m_ListenSocketList.end(); ++pos)
    {
        lpngx_connection_t p_Conn = ngx_get_connection((*pos)->fd);     // 从连接池中获取一个空闲的连接对象
        if(p_Conn == NULL)
        {
            // 这是致命问题，刚开始怎么可能连接池就为空呢？
            ngx_log_stderr(errno,"CSocket::ngx_epoll_init()中ngx_get_connection()失败.");
            exit(2); // 致命问题，直接退，交给系统处理释放
        }
        p_Conn->listening = (*pos);      // 连接对象 和 监听对象关联，方便通过连接对象找到监听对象
        (*pos)->connection = p_Conn;     // 监听对象 和 连接对象关联，方便通过监听对象找到连接对象

        // rev->accept = 1;         // 监听端口必须设置accept标志为1

        // 对于监听端口的读事件设置处理方法，因为监听端口是用来等待对象连接的发送三次握手的，所以监听端口关心的就是【读事件】
        p_Conn->rhandler = &CSocket::ngx_event_accept;

        // 往监听socket上增加监听事件，从而开始让监听端口履行其职责【如果不加这行，虽然端口能连接上，但是不会触发ngx_epoll_process_events()里面的epoll_wait()往下走
        // ngx_epoll_add_event参数
        // (*pos)->fd,              socket句柄
        // 1, 0,                    读， 写 【只关心读事件，所以参数2：readevent = 1，而参数3：writeevent = 0;】
        // 0,                       其他事件补充标记
        // EPOLL_CTL_ADD,           事件类型【增加，还有其他事件 MOV(修改),DEL(删除)】
        // c                        连接池中的连接
        // if(ngx_epoll_add_event((*pos)->fd, 1, 0, 0, EPOLL_CTL_ADD, c) == -1)
        // (*pos)->fd,              socket句柄
        // EPOLL_CTL_ADD,           事件类型，这里是增加
        // EPOLLIN|EPOLLRDHUP,      标志，这里代表要增加的标志，EPOLLIN:可读，EPOLLRDHUP:TCP连接的远端关闭或者半关闭
        // 0,                       对于事件类型为增加的，不需要这个参数
        // p_Conn                   连接池中的连接
        if(ngx_epoll_oper_event((*pos)->fd, EPOLL_CTL_ADD, EPOLLIN|EPOLLRDHUP, 0, p_Conn) == -1)
        {
            exit(2); //有问题，直接退出，日志 已经写过了
        }
    }
    
    return 1;

}

```

我们以往的代码，是在ngx_epoll_init()函数里进行连接池初始化，仔细看如下这个函数里这块被注释了的代码，可以发现这种写法比较受限，比如我们允许你1W个连接进来，比如当前就有一万个连接连入，这时候有一个用户掉线了，按照我们延迟回收连接的思想，此时再进来一个用户他将会得不到连接池可用连接进行服务，因为要等上一个离线连接延迟60s后释放回收连接完成，这个新用户才能接入。简单来说，就是这个连接没办法被新连入用户立即使用，我也没办法对新连入用户立即提供服务。

```c++
// m_connection_n = m_worker_connections;      // 记录当前连接池中的连接总数
    // // 连接池【数组，每个元素时一个对象】
    // m_pconnections = new ngx_connection_t[m_connection_n];      // new是不可能失败，这里不用判断，如果失败，直接报异常会更好
    // // m_pread_events = new ngx_event_t[m_connection_n];
    // // m_pwrite_events = new mgx_event_t[m_connection_n];
    // // for(int i = 0; i < m_connection_n; i++)
    // // {
    // //     m_pconnections[i].instance = 1; // 失效标志位设置为1,
    // // }

    // int i = m_connection_n;             // 连接池中的连接数
    // lpngx_connection_t next = NULL;
    // lpngx_connection_t c = m_pconnections;  // 连接池数组的首地址

    // // 这个do while循环就是在初始化连接池，并将连接池数组元素通过next指针绑定到一起，形成链表
    // do
    // {
    //     i--;                    // 注意i是数字末尾，从最后遍历，i递减至数组的首个元素

    //     // 从尾部往头部走----------------------
    //     c[i].data = next;       // 设置连接对象的next指针，注意第一次循环时next = NULL
    //     c[i].fd = -1;           // 初始化连接，无socket和该连接池中的连接【对象】绑定
    //     c[i].instance = 1;      // 失效标志位设置为1,【失效】
    //     c[i].iCurrsequence = 0; // 当前序号统一从 0 开始

    //     // -----------------------------------

    //     next = &c[i];           // next指针向前移

    // } while (i);    // 循环直到 i 为 0 .即循环到数组首地址
    // // 注意这里：当这个循环执行完毕后，next的指向现在是指向这个链表的表头

    // m_pfree_connections = next;         // 设置空闲连接链表头指针，因为现在next指向c[0]，注意现在整个链表都是空的
    // m_free_connection_n = m_connection_n;   // 空闲连接链表的长度，因为现在整个链表都是空的，所以这两个参数相等
```

所以基于这种场景，我们现在的连接池不再把连接数量定死，我们的连接池连接数可以慢慢增长。而且这个增长我们后续不再放开，即使没有那么多连接使用，也不再回调回去，而是一直增长到一个最大值，此后我们将保持这个最大值，除非系统退出。

初始化连接池这里使用了一个全新的实现：

这里有个值得注意的点。首先我们初始化连接的时候，给每一个连接都进行了内存初始化。因为我这里生成一个连接池连接lpngx_connection_t是使用自己封装的AllocMemory函数进行内存分配的，来看这个函数里面是怎么分配内存的

```c++
// 分配内存
// memsCount: 分配的字节大小
// ifmemset：是否要把分配的内存初始化为0
void *CMemory::AllocMemory(int memCount, bool ifmemset)
{
    void *temDate = (void *)new char[memCount];     // 不对new是否成功进行判断，如果new失败，程序就不应该继续运行，就让他崩溃以方便我们排错
    if (ifmemset)
    {
        memset(temDate, 0, memCount);
    }
    return temDate;
    
}
```

AllocMemory分配是使用new char来分配，那我本身这个连接池连接lpngx_connection_t是一个类，一个结构，在这个结构里我添加了一个构造函数和析构函数，那我就希望在初始化对象的时候调用构造函数，因为这种分配内存的方式没办法调用构造函数，所以在下一行使用了【定位new】语法，这个【定位new】不是分配内存，分配内存已经在AllocMemory中分配好了，【定位new】是以这个`p_Conn`为首地址调用构造函数的。

```c++
// 初始化连接池
void CSocket::initconnection()
{
    lpngx_connection_t p_Conn;
    CMemory *p_memory = CMemory::GetInstance();

    int ilenconnpool = sizeof(ngx_connection_t);

    for (int i = 0; i < m_worker_connections; ++i)      // 先创建这么多个连接，后续不够再增加
    {
        p_Conn = (lpngx_connection_t)p_memory->AllocMemory(ilenconnpool, true);     // 清理内存，因为这里分配内存new char，无法执行构造函数，所以如下：
        // 手工调用构造函数，因为AllocMemory里面无法调用构造函数
        p_Conn = new(p_Conn) ngx_connection_t();        // 定位new。释放则显示调用p_Conn->~ngx_connection_t();
        p_Conn->GetOneToUse();
        m_connectionList.push_back(p_Conn);             // 所有连接【不管是否空闲】都放在这个list
        m_freeconnectionList.push_back(p_Conn);         // 空闲连接会放在这个list里
    }

    m_free_connection_n = m_total_connection_n = m_connectionList.size();       // 开始这两个列表一样大
    return;
    
}
```

然后再往下看， `p_Conn->GetOneToUse();`这个函数主要是用来初始化一些数据，因为我们这个内存（连接）时需要重复使用的，不可能不断的new delete，所以一些初始化工作就用这个函数来进行。

```cc
// 分配出去一个连接的时候初始化一些内容，原来内容放在 ngx_get_connection() 里，现在放在这里
void ngx_connection_s::GetOneToUse()
{
    ++iCurrsequence;	// 每次调用该函数都把这个序号+1（也就是这个连接每被用一次，这个序号就+1）

    curStat = _PKG_HD_INIT;                                     // 收包状态处于初始状态，准备接收数据包头【状态机】
    precvbuf = dataHeadInfo;                                    // 收包要先收到这里来，因为要先收包头，所以收数据的buff直接就是dataHeadInfo
    irecvlen = sizeof(COMM_PKG_HEADER);                         // 这里指定收数据的长度，这里先要求收包头这么长字节的数据

    precvMemPointer = NULL;                                     // 既然没有new内存，那么自然指向的内存地址先给NULL
    iThrowsendCount = 0;                                        // 原子操作
    psendMemPointer = NULL;                                     // 发送数据头指针记录
    events          = 0;                                        // epoll事件先给0


}
```

然后`m_connectionList`和`m_freeconnectionList`分别是两个定义好的容器，分别存放来连接池所有连接和空闲可分配的连接

```c++
std::list<lpngx_connection_t>   m_connectionList;                   // 连接列表【连接池】
std::list<lpngx_connection_t>   m_freeconnectionList;               // 空闲连接列表【这里面装的全都是空闲的连接】
        
```

初始化完成之后，来了一个新用户，新用户使用ngx_get_connection这个函数来获取一个空闲连接。之前获取空闲连接的思想是直接取出空闲连接链表的表头作为空闲连接返回使用，那在引入了延迟回收的概念后，就需要重新这块逻辑：首先先对m_connectionMutex这个互斥量进行临界，主要是为了m_freeconnectionList和m_connectionList这两个链表做线程安全控制，因为其他线程函数也会操作这两个数据，所以需要做临界处理。然后判断空闲链表是否有值，有的话直接取出头部作为连接返回，并将统计计数-1，没有的话，就出现创建一个连接，然后将创建好的连接翻到总表中（注意这里不能放到空闲表中，因为你要调用这个函数，就代表你要使用这个连接，那他就是相当于被预定了，已经不是空闲的了）

```c++
// 从连接池中获取一个空闲连接，【当一个客户端tcp连接进入，我希望把这个连接和我连接池中的一个连接【对象】绑到一起，后续可以通过这个连接，把这个对象找到，因为对象里可以记录各种信息】
lpngx_connection_t CSocket::ngx_get_connection(int isock)
{
    // 因为可能有其他线程要访问m_freeconnectionList,m_connectionList【比如可能有专门的释放线程要释放/或者主线程要释放】之类的，所以这里需要临界处理
    CLock lock(&m_connectionMutex);

    if (!m_freeconnectionList.empty())
    {
        // 有空闲的，自然是从空闲列表中拿取
        lpngx_connection_t p_Conn = m_freeconnectionList.front();           // 返回第一个元素但不检查元素是否存在
        m_freeconnectionList.pop_front();                                   // 移除第一个元素但不返回
        p_Conn->GetOneToUse();
        --m_free_connection_n;
        p_Conn->fd = isock;
        return p_Conn;
    }
    
    // 走到这里，表示没有空闲的连接了，那就考虑重新创建一个连接
    CMemory *p_memory = CMemory::GetInstance();
    lpngx_connection_t p_Conn = (lpngx_connection_t)p_memory->AllocMemory(sizeof(ngx_connection_t), true);
    p_Conn = new(p_Conn) ngx_connection_t();
    p_Conn->GetOneToUse();
    m_connectionList.push_back(p_Conn);             // 放到总表中来，但是不能放入空闲表中，因为凡是调用这个函数的，肯定要用这个连接的
    ++m_total_connection_n;
    p_Conn->fd = isock;
    return p_Conn;
}
```



## 连接池中连接的回收

**a)立即回收【accept用户没有接入时可以立即回收】** 

立即回收 ngx_free_connection()；

```c++
// 归还参数p_Conn所代表的连接到连接池中，注意参数类型是lpngx_connection_t
void CSocket::ngx_free_connection(lpngx_connection_t p_Conn)
{

    // 因为有线程可能要动连接池中的连接，所以在这里互斥也是有必要的
    CLock lock(&m_ConnectionMutex);

    // 首先先明确一点，连接，所有连接全部都在m_connectionList里面
    p_Conn->PutOneToFree();

    // 扔到空闲连接列表里
    m_freeconnectionList.push_back(p_Conn);

    // 空闲连接数+1
    ++m_free_connection_n;
    
    return;

}
```

回收的时候调用了PutOneToFree函数。可以看到不管是回收还是分配，iCurrsequence这个计数都会+1，表示这个连接不再是原来的它，是已经被使用过的连接

```c++
// 回收回来一个连接的时候做一些事
void ngx_connection_s::PutOneToFree()
{
    ++iCurrsequence;

    if (precvMemPointer != NULL)        // 之前给这个连接分配过接收数据的内存，现在就需要进行释放内存
    {
        CMemory::GetInstance()->FreeMemory(precvMemPointer);
        precvMemPointer = NULL;
    }
    if (psendMemPointer != NULL)        // 如果发送数据的缓冲区有内容，则要释放内存
    {
        CMemory::GetInstance()->FreeMemory(psendMemPointer);
        psendMemPointer = NULL;
    }
    
    iThrowsendCount = 0;
    
}

```

然后这个函数里提到给这个连接分配过接收数据的内存，那这个内存是怎么来的？可以想到，在收包的时候，如果收到一半包，那么我们会把收包的首地址给到pConn->precvMempointer。后续收包收完整了，那么这个内存会被放到ngx_wait_request_handler_proc_plast函数里去，哪里会对这个内存进行释放，但是如果我们收包不全，然后链接断开了，那么就很有必要要把这个连接里的这个内存块清理干净（同样，对于发包也是一样的道理）

**b)延迟回收【用户接入进来开始干活了】；一旦你accept把这个用户接入进来了，通过了三次握手把这个连接绑定到socket上了，开始收发数据了，那就得延迟回收**

延迟回收 inRecyConnectQueue()，

```c++
// 将要回收的连接放到一个队列中来，后续有专门的线程会处理这个队列中的连接的回收
// 有些连接，我们不希望马上进行释放，而是要隔一段时间后再进行进行释放以确保服务器的稳定，所以，我们把这种隔一段时间才释放的连接放到一个队列中来
void CSocket::inRecyConnectQueue(lpngx_connection_t pConn)
{
    // ngx_log_stderr(0,"CSocekt::inRecyConnectQueue()执行，连接入到回收队列中.");
    CLock lock(&m_recyconnqueueMutex);          // 针对连接回收列表的互斥量，以线程ServerRecyConnectionThread()也要用到这个回收列表

    pConn->inRecyTime = time(NULL);             // 记录回收时间
    ++pConn->iCurrsequence;
    m_recyconnectionList.push_back(pConn);      // 等待ServerRecyConnectionThread线程自会处理
    ++m_total_recyconnection_n;                 // 等待释放连接队列大小+1
    return;
}
```

这个互斥量`m_recyconnqueueMutex`是专门针对延迟回收列表的，

ServerRecyConnectionThread()

在这个函数的while循环里，有个判断`if (pSocketObj->m_total_recyconnection_n > 0)`，判断该连接池的垃圾待回收连接数是否大于0，如果大于，说明有待回收的连接。然后使用迭代器把每个待回收连接取出来，判断放入这个连接的开始时间+我们在配置文件中预定义好的等待（延迟回收）秒数是否大于当前时间，如果大于，就说明这条连接**还没到该回收的时间**，同时判断程序退出标志，如果不要求退出（等于0），那就continue迭代下一个待回收连接。否者就说明可以释放，然后对计数-1，释放当前迭代器，调用`ngx_free_connection`立即释放当前连接

```c++
// 处理连接回收的线程
void *CSocket::ServerRecyConnectionThread(void * threadData)
{
    ThreadItem *pThread = static_cast<ThreadItem*>(threadData);
    CSocket *pSocketObj = pThread->_pThis;

    time_t currtime;
    int err;
    std::list<lpngx_connection_t>::iterator pos,posend;
    lpngx_connection_t p_Conn;

    while (1)
    {
        // 为了简化问题，我们直接每次休息200ms
        usleep(200*1000);   // 单位是微秒：又因为一毫秒等于1000微秒，所以 200*1000 = 200毫秒

        // 不管什么情况，先把这个条件成立时该做的动作做了
        if (pSocketObj->m_total_recyconnection_n > 0)
        {
            currtime = time(NULL);
            err = pthread_mutex_lock(&pSocketObj->m_recyconnqueueMutex);
            if (err != 0)
            {
                ngx_log_stderr(err,"CSocekt::ServerRecyConnectionThread()中pthread_mutex_lock()失败，返回的错误码为%d!",err);
            }

lblRRTD:
            pos     = pSocketObj->m_recyconnectionList.begin();
            posend  = pSocketObj->m_recyconnectionList.end()
            for (; pos != posend; ++pos)
            {
                p_Conn = (*pos);

                // 如果不是要整个系统退出，可以continue，否者就得强制释放
                if (((p_Conn->inRecyTime + pSocketObj->m_RecyConnectionWaitTime) > currtime) && (g_stopEvent == 0))
                {
                    continue;       // 没到释放时间
                }

                // 到释放时间
                // ...将来这里可能还要做一些是否能释放的判断

                // 流程走到这里，表示可以释放，那就开始释放
                --pSocketObj->m_total_recyconnection_n;         // 待释放连接队列大小-1
                pSocketObj->m_recyconnectionList.erase(pos);    // 迭代器已经失效，但是pos所指向的内容在p_Conn里保存着

                // ngx_log_stderr(0,"CSocekt::ServerRecyConnectionThread()执行，连接%d被归还.",p_Conn->fd);
                
                pSocketObj->ngx_free_connection(p_Conn);        // 归还参数pConn所代表的连接到连接池中
                goto lblRRTD;
                
            }
            
            err = pThread_mutex_unlock(&pSocketObj->m_recyconnqueueMutex);
            if (err != 0)
            {
                ngx_log_stderr(err,"CSocket::ServerRecyConnectionThread()pthread_mutex_unlock()失败，返回的错误码为%d!",err);
            }
        }

        if (g_stopEvent == 1)   // 要退出整个程序，那么肯定要先退出这个循环
        {
            if (pSocketObj->m_total_recyconnection_n > 0)
            {
                // 因为要退出，所以就得硬释放了【不管有没有到时间，不管有没有其他不允许释放的连接，都得硬释放】
                err = pthread_mutex_lock(&pSocketObj->m_recyconnqueueMutex);
                if(err != 0) ngx_log_stderr(err,"CSocket::ServerRecyConnectionThread()中pthread_mutex_lock2()失败，返回的错误码为%d!",err);

                lblRRTD2:
                pos     = pSocketObj->m_recyconnectionList.begin();
                posend  = pSocketObj->m_recyconnectionList.end();
                for (; pos != posend; ++pos)
                {
                    p_Conn = (*pos);
                    --pSocketObj->m_total_recyconnection_n;             // 待释放连接队列大小-1
                    pSocketObj->m_recyconnectionList.erase(pos);        // 迭代器已经失效，但是pos所指向的内容在p_Conn里保存着
                    pSocketObj->ngx_free_connection(p_Conn);            // 归还参数pConn所代表的连接到连接池中
                    goto lblRRTD2;
                }
                
                err = pthread_mutex_unlock(&pSocketObj->m_recyconnqueueMutex);
                if(err != 0)  ngx_log_stderr(err,"CSocket::ServerRecyConnectionThread()pthread_mutex_unlock2()失败，返回的错误码为%d!",err);
            }
            break;  // 因为整个程序要退出了，所以这里直接break
            
        }
        
    }
    
    return (void*)0;
}

```



# 程序退出时线程的安全终止

多线程开发技术在我们这个项目中是全面开花的

大概思路，将程序退出这个全局量g_stopEvent置为1；

然后手工调用shutdown_subproc()

# epoll事件处理的改造

## 增加新的事件处理函数

引入ngx_epoll_oper_event()函数取代ngx_epoll_add_event()；

```c++
// 对于epoll事件的具体操作
// 返回值：成功返回1，失败返回-1
/**
 * 参数列表：
 * int fd                           句柄，一个socket
 * uint32_t eventtype               事件类型，一般是EPOLL_CTL_ADD, EPOLL_CTL_MOD, EPOLL_CTL_DEL 说白了就是操作epoll红黑树的节点（增加，修改，删除）
 * uint32_t flag                    标志，具体含义取决于eventtype
 * int bcaction                     补充动作，用于补充flag标记的不足 ： 0 增加 ； 1  去掉
 * lpngx_connection_t pConn         一个指针【实际上是一个连接】EPOLL_CTL_ADD的时候增加到红黑树中去，将来在epoll_wait的时候能取出来
 * */
int CSocket::ngx_epoll_oper_event(int fd, uint32_t eventtype, uint32_t flag, int bcaction, lpngx_connection_t pConn)
{
    struct epoll_event ev;
    memset(&ev, 0, sizeof(ev));

    if (eventype == EPOLL_CTL_ADD)  // 往红黑树中加节点
    {
        // 红黑树中从无到有增加节点
        ev.data.ptr = (void*)pConn;
        ev.events = flag;               // 既然是增加节点，则不管原来是什么标记
        pConn->events = flag;           // 这个连接本身也记录这个标记
    }
    else if (eventtype == EPOLL_CTL_MOD)
    {
        // 节点已经在红黑树中，修改节点的事件信息
    }
    else
    {
        // 删除红黑树中节点，目前没这个需求，所以将来再拓展
        return 1;          // 先直接返回1表示成功
    }

    if (epoll_ctl(m_epollhandle, eventtype, fd, &ev) == -1)
    {
        ngx_log_stderr(errno,"CSocekt::ngx_epoll_oper_event()中epoll_ctl(%d,%ud,%ud,%d)失败.",fd,eventtype,flag,bcaction);    
        return -1;
    }

    return 1;
    
    
}

```

参数eventtype表示往红黑树中增加节点，后续往外发送数据的话，还要增加发送时间，修改红黑树，一般不需要我们删除节点，在客户端断开连接的时候，系统会帮助我们自动删除。这个函数什么时候调用，比如三次握手连接进来，这个时候我们要增加读事件，然后后面那个参数EPOLLIN|EPOLLRDHUP表示可读的时候和断开的时候添加监听

## 调整对事件处理函数的调用

ngx_epoll_init(),ngx_event_accept(),ngx_epoll_process_events();

# 连接延迟回收的具体应用

recvproc()调用了inRecyConnectQueue(延迟回收)取代了ngx_close_connection(立即回收)
Initialize_subproc()子进程中干；
Shutdown_subproc()子进程中干

──master process ./nginx 

──worker process

───ngx_master_process_cycle()     //创建子进程等一系列动作

────ngx_setproctitle()       //设置进程标题   

────ngx_start_worker_processes()  //创建worker子进程  

─────for (i = 0; i < threadnums; i++)  //master进程在走这个循环，来创建若干个子进程

──────ngx_spawn_process(i,"worker process");

───────pid = fork(); //分叉，从原来的一个master进程（一个叉），分成两个叉（原有的master进程，以及一个新fork()出来的worker进程

─────── //只有子进程这个分叉才会执行ngx_worker_process_cycle()

───────ngx_worker_process_cycle(inum,pprocname);  //子进程分叉

────────ngx_worker_process_init();

─────────sigemptyset(&set);  

─────────sigprocmask(SIG_SETMASK, &set, NULL); //允许接收所有信号

─────────g_threadpool.Create(tmpthreadnums);  //创建线程池中线程

─────────_socket.Initialize_subproc();  //初始化子进程需要具备的一些多线程能力相关的信息

─────────g_socket.ngx_epoll_init();  //初始化epoll相关内容，同时 往监听socket上增加监听事件，从而开始让监听端口履行其职责

──────────m_epollhandle = epoll_create(m_worker_connections); 

──────────ngx_epoll_oper_event((*pos)->fd....);

───────────epoll_ctl(m_epollhandle,eventtype,fd,&ev);

────────ngx_setproctitle(pprocname);      //重新为子进程设置标题为worker process

────────for ( ;; ) {

─────────//子进程开始在这里不断的死循环

─────────ngx_process_events_and_timers();	// 处理网络事件和定时器事件

──────────g_socket.ngx_epoll_process_events(-1); //-1表示卡着等待吧

───────────**ngx_socket_epoll_process_events(-1);**	// -1表示卡着等待

────────}

────────g_threadpool.StopAll();      //考虑在这里停止线程池；

────────g_socket.Shutdown_subproc(); //socket需要释放的东西考虑释放；



────sigemptyset(&set); 

────for ( ;; ) {}.         //父进程[master进程]会一直在这里循环



**注意，这里我们采用延迟回收，那就可能会出现过期事件**

如果遇到过期事件，这里采用的部分是按照正常事件进行处理，就当他没过期。比如a连接发送消息过来，正在处理事件，然后a连接断开，这个时候，等过了回收缓冲时间后，这个连接被回收，这个时候来了个B连接，他用了断开的A刚刚回收的这个连接，然后这个时候A的业务处理完了，他会给B发送A的处理结果数据吗？（张冠李戴），其实是不会的，因为在A连接断开的时候，他的序号++pConn->iCurrsequeue是会+1的，在处理完业务，往回发包的时候，是会将这个值与消息头中的这个值进行比较，如果不等，那就说明是个过期包，就不会把这个包往现任连接套接字上发。